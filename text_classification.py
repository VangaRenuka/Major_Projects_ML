# -*- coding: utf-8 -*-
"""Text_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19qw8VztDZCYPM4EqWIJH028-NgDav8qN

**Text Classification**
"""

# Install necessary libraries
!pip install -U scikit-learn gensim transformers tensorflow
# scikit-learn:A popular library for machine learning tasks.Provides tools for data preprocessing, classification, regression, clustering, and evaluation metrics.
# gensim:used for creating word embeddings (e.g., Word2Vec) and working with text data.
# transformers:Allows tokenization, fine-tuning, and inference using pre-trained models.
# tensorflow:A widely-used library for deep learning and machine learning.
# Example use: Extracting features from text data using TensorFlow-based BERT models.

# Import libraries
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer #Converts raw text into numerical vectors using TF-IDF (Term Frequency-Inverse Document Frequency
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from gensim.models import Word2Vec
from transformers import BertTokenizer, TFBertModel #Prepares text for input into the BERT model by splitting it into tokens and converting them to IDs    Extracts meaningful features from text for downstream tasks like classification or question answering.
import tensorflow as tf   #Provides the tools needed to create, train, and evaluate neural networks.
import numpy as np

"""Part 1: Classification with TF-IDF Document Representation"""

# Load dataset
data = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)
texts, labels = data.data, data.target

import csv

# Save texts and labels into a CSV file
with open("dataset.csv", "w", newline='', encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["Text", "Label"])  # Header row
    writer.writerows(zip(texts, labels))

# TF-IDF Vectorization
tfidf = TfidfVectorizer(stop_words='english', max_features=5000)
X = tfidf.fit_transform(texts)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler(with_mean=False)  # With mean=False since the matrix is sparse
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# TF-IDF Reduced Vectorization
tfidf_reduced = TfidfVectorizer(stop_words='english', max_features=1500,ngram_range=(1, 2))
X_reduced = tfidf_reduced.fit_transform(texts)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_reduced, labels, test_size=0.2, random_state=42)

# Logistic Regression
lr = LogisticRegression(max_iter=200,class_weight='balanced')
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
print("TF-IDF Reduced Logistic Regression Accuracy:", accuracy_score(y_test, lr_pred))

# Support Vector Machine (SVM)
svm = SVC()
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
print("TF-IDF Reduced SVM Accuracy:", accuracy_score(y_test, svm_pred))

X_train = X_train.toarray() + np.random.normal(0, 0.1, X_train.shape)
svm=SVC(kernel='linear', C=0.1)  # Modified kernel and C
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
print("TF-IDF Reduced SVM Accuracy:", accuracy_score(y_test, svm_pred))

# Random Forest
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
print("TF-IDF Reduced Random Forest Accuracy:", accuracy_score(y_test, rf_pred))

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
print("TF-IDF Reduced KNN Accuracy:", accuracy_score(y_test, knn_pred))

# Naive Bayes
nb = MultinomialNB()
nb.fit(X_train, y_train)
nb_pred = nb.predict(X_test)
print("TF-IDF Reduced Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))

# Decision Tree
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)
print("TF-IDF Reduced Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))

# TF-IDF Reduced Vectorization
tfidf_reduced = TfidfVectorizer(stop_words='english', max_features=1500,ngram_range=(1, 2))
X_reduced = tfidf_reduced.fit_transform(texts)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_reduced, labels, test_size=0.2, random_state=42)

# Logistic Regression
lr = LogisticRegression(max_iter=200,class_weight='balanced')
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
print("TF-IDF Reduced Logistic Regression Accuracy:", accuracy_score(y_test, lr_pred))

# Support Vector Machine (SVM)
# Convert X_train to dense before training to ensure consistency with X_test if needed
X_train_dense = X_train.toarray()
#Add noise if you intend to
X_train_dense = X_train_dense + np.random.normal(0, 0.1, X_train_dense.shape)

svm = SVC(kernel='linear', C=0.1)  # Modified kernel and C
svm.fit(X_train_dense, y_train)  # Train on dense data

#For prediction, you also have to convert X_test to dense since it was sparse in the training phase
X_test_dense = X_test.toarray()

svm_pred = svm.predict(X_test_dense)  # Predict on dense data
print("TF-IDF Reduced SVM Accuracy:", accuracy_score(y_test, svm_pred))

# Random Forest
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
print("TF-IDF Reduced Random Forest Accuracy:", accuracy_score(y_test, rf_pred))

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
print("TF-IDF Reduced KNN Accuracy:", accuracy_score(y_test, knn_pred))

# Naive Bayes
nb = MultinomialNB()
nb.fit(X_train, y_train)
nb_pred = nb.predict(X_test)
print("TF-IDF Reduced Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))

# Decision Tree
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)
print("TF-IDF Reduced Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))

# TF-IDF Complete Vectorization
tfidf_complete = TfidfVectorizer(stop_words='english')  # No max_features limitation
X_complete = tfidf_complete.fit_transform(texts)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_complete, labels, test_size=0.2, random_state=42)

# Logistic Regression
lr = LogisticRegression(max_iter=200)
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
print("TF-IDF Complete Logistic Regression Accuracy:", accuracy_score(y_test, lr_pred))

# Support Vector Machine (SVM)
svm = SVC()
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
print("TF-IDF Complete SVM Accuracy:", accuracy_score(y_test, svm_pred))

# Random Forest
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
print("TF-IDF Complete Random Forest Accuracy:", accuracy_score(y_test, rf_pred))

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
print("TF-IDF Complete KNN Accuracy:", accuracy_score(y_test, knn_pred))

# Naive Bayes
nb = MultinomialNB()
nb.fit(X_train, y_train)
nb_pred = nb.predict(X_test)
print("TF-IDF Complete Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))

# Decision Tree
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)
print("TF-IDF Complete Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))

"""Part 2: Classification with Word2Vec Document Representation
Word2Vec Preprocessing
"""

# Prepare text data for Word2Vec
texts_tokenized = [text.split() for text in texts]
word2vec_model = Word2Vec(sentences=texts_tokenized, vector_size=300, window=5, min_count=2, workers=4)
# sentences=texts_tokenized: Tokenized sentences are passed to train the Word2Vec model.
# vector_size=100: Specifies the dimensionality of the word embeddings (i.e., each word will be represented as a vector of size 100).
# window=5: The size of the context window; the model looks at 5 words before and after a target word.
# min_count=2: Ignores words that appear less than 2 times in the corpus.
# workers=4: Uses 4 threads for training to speed up computation.

# Generate document embeddings by averaging word embeddings
def get_document_embedding(doc):
    words = [word for word in doc.split() if word in word2vec_model.wv]
    return np.mean(word2vec_model.wv[words], axis=0) if words else np.zeros(100)

X = np.array([get_document_embedding(doc) for doc in texts])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

"""Train and Evaluate Models with Word2Vec"""

# Logistic Regression
lr = LogisticRegression(max_iter=200)
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
print("Word2Vec Logistic Regression Accuracy:", accuracy_score(y_test, lr_pred))

# SVM
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
print("Word2Vec SVM Accuracy:", accuracy_score(y_test, svm_pred))

# Random Forest
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
print("Word2Vec Random Forest Accuracy:", accuracy_score(y_test, rf_pred))

# KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
print("Word2Vec KNN Accuracy:", accuracy_score(y_test, knn_pred))

from sklearn.naive_bayes import GaussianNB

# Use Gaussian Naive Bayes for Word2Vec for negative embedings
nb = GaussianNB()
nb.fit(X_train, y_train)
nb_pred = nb.predict(X_test)
print("Word2Vec Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)
print("Word2Vec Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))


# naviy bais
##from sklearn.naive_bayes import MultinomialNB
#nb = MultinomialNB()
#nb.fit(X_train, y_train)
#nb_pred = nb.predict(X_test)
#print("Word2Vec Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train, y_train)
nb_pred = nb.predict(X_test)
print("Word2Vec Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))

"""Part 3: Classification with BERT Document Representation
BERT Preprocessing
"""

from transformers import BertTokenizer, TFBertModel

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = TFBertModel.from_pretrained("bert-base-uncased")

# Loads a pre-trained BERT model (bert-base-uncased) from the Hugging Face model hub.
# bert-base-uncased:
# Base: A version of BERT with 12 layers, 768 hidden units, and 12 attention heads.
# Uncased: Converts all text to lowercase and ignores case distinctions.
# Tokenizer:
# Used to tokenize input text into subwords and encode them into numerical IDs suitable for BERT.
# Model:
# TFBertModel returns the hidden states (embeddings) of the input text after processing through BERT layers.

# Tokenize and encode
##inputs = tokenizer(texts, return_tensors="tf", padding=True, truncation=True, max_length=128)
#outputs = model(inputs)

# Extract CLS embeddings
#X = outputs.last_hidden_state[:, 0, :].numpy()
# Split data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

inputs = tokenizer(texts, return_tensors="tf", padding=True, truncation=True, max_length=128)

batch_size = 32  # Adjust based on your GPU capacity
embeddings = []

for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]
    inputs = tokenizer(batch_texts, return_tensors="tf", padding=True, truncation=True, max_length=128)
    outputs = model(inputs)
    batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()
    embeddings.append(batch_embeddings)

X = np.concatenate(embeddings, axis=0)

from transformers import DistilBertTokenizer, TFDistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")

from tensorflow.keras.mixed_precision import set_global_policy

set_global_policy('mixed_float16')  # Enable mixed precision

from transformers import BertTokenizer, TFBertModel
import numpy as np

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = TFBertModel.from_pretrained("bert-base-uncased")

# Process inputs in batches
batch_size = 32
max_length = 128
embeddings = []

for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]
    inputs = tokenizer(batch_texts, return_tensors="tf", padding=True, truncation=True, max_length=max_length)
    outputs = model(inputs)
    batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()
    embeddings.append(batch_embeddings)

X = np.concatenate(embeddings, axis=0)
BX_train, BX_test, By_train, By_test = train_test_split(X, labels, test_size=0.2, random_state=42)

"""Train and Evaluate Models with BERT"""

# Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(max_iter=200)
lr.fit(BX_train, By_train)
lr_pred = lr.predict(BX_test)
print("BERT Logistic Regression Accuracy:", accuracy_score(y_test, lr_pred))

# SVM
svm.fit(BX_train, By_train)
svm_pred = svm.predict(BX_test)
print("BERT SVM Accuracy:", accuracy_score(By_test, svm_pred))

# Random Forest
rf.fit(BX_train, By_train)
rf_pred = rf.predict(BX_test)
print("BERT Random Forest Accuracy:", accuracy_score(By_test, rf_pred))

# KNN
knn.fit(BX_train, By_train)
knn_pred = knn.predict(BX_test)
print("BERT KNN Accuracy:", accuracy_score(By_test, knn_pred))

# Naive Bayes
nb.fit(BX_train, By_train)
nb_pred = nb.predict(BX_test)
print("BERT Naive Bayes Accuracy:", accuracy_score(By_test, nb_pred))

# Decision Tree
dt.fit(BX_train, By_train)
dt_pred = dt.predict(BX_test)
print("BERT Decision Tree Accuracy:", accuracy_score(By_test, dt_pred))

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Bidirectional, Embedding, Input, GlobalAveragePooling1D
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import accuracy_score

# One-hot encode the labels
encoder = LabelBinarizer()
y_train_encoded = encoder.fit_transform(y_train)
y_test_encoded = encoder.transform(y_test)

"""Multilayer Perceptron (MLP)"""

def build_mlp(input_shape, num_classes):
    model = Sequential([
        Dense(512, activation='relu', input_shape=(input_shape,)),
        Dropout(0.5),
        Dense(256, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

mlp_model = build_mlp(X_train.shape[1], y_train_encoded.shape[1])
mlp_model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)
mlp_predictions = mlp_model.predict(X_test)
mlp_accuracy = accuracy_score(y_test, mlp_predictions.argmax(axis=1))
print("MLP Accuracy:", mlp_accuracy)

"""Convolutional Neural Network (CNN)"""

# def build_cnn(input_shape, num_classes):
#     model = Sequential([
#         Input(shape=(input_shape, 1)),
#         Conv1D(128, kernel_size=5, activation='relu'),
#         MaxPooling1D(pool_size=2),
#         Dropout(0.5),
#         Conv1D(64, kernel_size=5, activation='relu'),
#         MaxPooling1D(pool_size=2),
#         Flatten(),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(num_classes, activation='softmax')
#     ])
#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
#     return model

# X_train_cnn = X_train[..., None]  # Add channel dimension for Conv1D
# X_test_cnn = X_test[..., None]
# cnn_model = build_cnn(X_train_cnn.shape[1], y_train_encoded.shape[1])
# cnn_model.fit(X_train_cnn, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)
# cnn_predictions = cnn_model.predict(X_test_cnn)
# cnn_accuracy = accuracy_score(y_test, cnn_predictions.argmax(axis=1))
# print("CNN Accuracy:", cnn_accuracy)

"""Long Short-Term Memory (LSTM)"""

# def build_lstm(input_shape, num_classes):
#     model = Sequential([
#         Input(shape=(input_shape, 1)),
#         LSTM(128, return_sequences=False),
#         Dropout(0.5),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(num_classes, activation='softmax')
#     ])
#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
#     return model

# X_train_lstm = X_train[..., None]
# X_test_lstm = X_test[..., None]
# lstm_model = build_lstm(X_train_lstm.shape[1], y_train_encoded.shape[1])
# lstm_model.fit(X_train_lstm, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)
# lstm_predictions = lstm_model.predict(X_test_lstm)
# lstm_accuracy = accuracy_score(y_test, lstm_predictions.argmax(axis=1))
# print("LSTM Accuracy:", lstm_accuracy)

"""Bidirectional LSTM"""

# def build_bilstm(input_shape, num_classes):
#     model = Sequential([
#         Input(shape=(input_shape, 1)),
#         Bidirectional(LSTM(128, return_sequences=False)),
#         Dropout(0.5),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(num_classes, activation='softmax')
#     ])
#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
#     return model

# bilstm_model = build_bilstm(X_train_lstm.shape[1], y_train_encoded.shape[1])
# bilstm_model.fit(X_train_lstm, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)
# bilstm_predictions = bilstm_model.predict(X_test_lstm)
# bilstm_accuracy = accuracy_score(y_test, bilstm_predictions.argmax(axis=1))
# print("BiLSTM Accuracy:", bilstm_accuracy)

"""CNN + LSTM"""

# def build_cnn_lstm(input_shape, num_classes):
#     model = Sequential([
#         Input(shape=(input_shape, 1)),
#         Conv1D(128, kernel_size=5, activation='relu'),
#         MaxPooling1D(pool_size=2),
#         LSTM(128, return_sequences=False),
#         Dropout(0.5),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(num_classes, activation='softmax')
#     ])
#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
#     return model

# cnn_lstm_model = build_cnn_lstm(X_train_cnn.shape[1], y_train_encoded.shape[1])
# cnn_lstm_model.fit(X_train_cnn, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)
# cnn_lstm_predictions = cnn_lstm_model.predict(X_test_cnn)
# cnn_lstm_accuracy = accuracy_score(y_test, cnn_lstm_predictions.argmax(axis=1))
# print("CNN+LSTM Accuracy:", cnn_lstm_accuracy)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Sample training data
training_texts = [
    "Pens fans are great supporters.",
    "I love the playoffs and hockey.",
    "Jersey team needs improvement.",
    "Bowman is an exceptional coach.",
    "The Islanders did well last season."
]
training_labels = [0, 1, 0, 1, 0]  # Example labels corresponding to categories

# Categories (replace with your own)
categories = ["Sports", "Entertainment", "Politics", "Technology"]

# Initialize and fit the TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()
tfidf_training_vectors = tfidf_vectorizer.fit_transform(training_texts)

# Train a Logistic Regression model
tfidf_model = LogisticRegression()
tfidf_model.fit(tfidf_training_vectors, training_labels)

# Input text to predict
input_text = """
From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>
Subject: Pens fans reactions
Organization: Post Office, Carnegie Mellon, Pittsburgh, PA
Lines: 12
NNTP-Posting-Host: po4.andrew.cmu.edu

I am sure some bashers of Pens fans are pretty confused about the lack
of any kind of posts about the recent Pens massacre of the Devils. Actually,
I am  bit puzzled too and a bit relieved. However, I am going to put an end
to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they
are killing those Devils worse than I thought. Jagr just showed you why
he is much better than his regular season stats. He is also a lot
fo fun to watch in the playoffs. Bowman should let JAgr have a lot of
fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final
regular season game.
"""
# Preprocess the input
processed_input = input_text.lower().replace("\n", " ").strip()

# Transform the input using TF-IDF
tfidf_input_vector = tfidf_vectorizer.transform([processed_input])

# Predict the category
tfidf_prediction = tfidf_model.predict(tfidf_input_vector)
print(f"TF-IDF Predicted Category: {categories[tfidf_prediction[0]]}")